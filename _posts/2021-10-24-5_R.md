---
layout: page
title: "5_R"
categories: researches
#permalink: /homeworks/researches/
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<b>5_R. Explain a possibly unified conceptual framework to obtain all most common measures of central tendency and of dispersion using the concept of distance (or "premetric", or similarity in general). Discuss why it is useful to discuss these concepts introducing the notion of distance. Finally, point out the difference between the mathematical definition of "distance" and the properties of the "premetrics" useful in statistics, pointing out trhe most important distances, indexes and similarity measures used in statistics, data analysis and machine learning (such as for instance; Mahalanobis distance, Euclidean distance, Minkowski distance, Manhattan distance, Hamming distance, Cosine distance, Chebishev distance, Jaccard index, Haversine distance, Sørensen-Dice index, etc.).</b>

In this hypothetical conceptual framework, where we may be interested in analyzing the frequency distribution of some variables, we are trying to get all most common measures of central tendency and of dispersion. A measure of central tendency in general indicates the value around which all the gathered values are laying. Examples of such measures are the mean, the median and the mode. 

One thing to notice is the fact that not for every type of variable we can obtain such measures. For example, we cannot have the mean for non-numerical values and we cannot have the median for values that are not orderable.

Given the central tendency, measures of dispersion give us an idea of how distant those values tend to be from the average. This can be known for example from measures like the standard deviation, the mean absolute deviation and the variance. Other important measures of dispersion are skewness, kurtosis, covariance and range.

In this conceptual framework, given a variable or a set of variables to analyze, the main measure to get would be the mean. In fact most of the measures of dispersion are a function of the mean (one example of a measure of dispersion not dependent from the mean is the range). Mean absolute value for example is the average distance of every point from the mean, while statistical distance and variance are similar concepts. Using the concept of skewness instead we can now not only the average distance from the mean but also which part of the whole set of values tend to be further from the mean, if the smallest half or the greatest. With a positively skewed distribution the values tend to be more concentrated on the left side, while the greatest value tend to stay further from the mean. It’s the opposite in the case of a negatively skewed distribution.

These informations are important because they give us an idea of the shape of the representing chart without the need to draw it. Knowing measures like the mean, the mode, the range, the kurtosis, the standard deviation and other similar measures we can know where those values are lying, if they are more concentrated around certain values or if they are more homogeneus.

Distance is a numerical measurement of how far apart objects or points are. It can be represented as a straight line between one point and another one. The way this line is drawn depends on the type of data we’re trying to measure. The most common distance measure is the Euclidean distance, which is length of a segment connecting two points in two and three-dimensional space. Other types of distances include the Manhattan distance, which is the distance between two vectors if they could only move right angles, the Chebyshev distance, which is simply the maximum distance along one axis, and the Minkowski distance, which is a generalization of the Chebyshev, Manhattan and Euclidean distances. The formula for the Minkowski distance is $$(\sum_{i=1}^{n} \vert x_{i} - y_{i} \vert^{p})^{1/p}$$. For p=1 we have the Manhattan distance, for p=2 we have the Euclidean distance and for p=$$\infty$$ we have the Chebyshev distance.

---------------------------------------------------------------------------------------

Sources:

https://en.wikipedia.org/wiki/Distance

https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa


