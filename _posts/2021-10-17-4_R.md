---
layout: page
title: "4_R"
categories: researches
#permalink: /homeworks/researches/
---
<b>4_R. Explain what are marginal, joint and conditional distributions and how we can explain the Bayes theorem using relative frequencies. Explain the concept of statistical independence and why, in case of independence, the relative joint frequencies are equal to the products of the corresponding marginal frequencies.</b>

In probability theory and statistics, the marginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables. This contrasts with a conditional distribution, which gives the probabilities contingent upon the values of the other variables.

A conditional probability distribution is a probability distribution for a sub-population. That is, a conditional probability distribution describes the probability that a randomly selected person from a sub-population has the one characteristic of interest.

A joint (bivariate) probability distribution describes the probability that a randomly selected statistical unit from the population has the two characteristics of interest.

The probability of a given phenomenon is directly connected with the observations of the phenomenon. From this comes the connection between statistics and probability theory. We can in fact apply statistics concepts like conditional and joint frequency distributions to probability theory. For example, having a bivariate distribution of two variables for a given population, we can say that a row of the corresponding contingency table can be used to express the conditional probability for the values of one of the variables, having established a priori (so, being conditioned on) the value of the other variable. Furthermore, each relative frequency of the contingency table expresses the probability that two specific values of those variables correspond to the same statistical unit. So, the joint frequency could be expressed in probabilistic terms as: P(X=x and Y=y) (for simplicity, we could just write it as P(X and Y)). Instead, the conditional frequency, expressed in probabilistic terms as P(X \| Y), is equal to the joint frequency of X and Y divided by the marginal probability of X. The marginal probability is just the probability of the event associated to a subset of all the variables. For instance here P(X) and P(Y) are associated to marginal distribution.

Given all those analogies, the Bayes’ theorem could be expressed as:

{% highlight plaintext %}
Conditional probability of X with value of Y fixed = Conditional probability of Y with value of X fixed * Marginal probability of X / Marginal probability of Y
{% endhighlight %}

Where the word “probability” can be exchanged with the word “frequency”.

Two events are independent, statistically independent, or stochastically independent if the occurrence of one does not affect the probability of occurrence of the other. Imagine throwing two dice. The result of one of the dice will not influence the output of the other one. In fact the probability of each result of the throwing is always the same, independently from the result of the previous throwing, and throwing two dice is equivalent to throwing one dice twice. So, for example, the probability P[X=5 and Y=2] is equal to P[X=5] * P[Y=2 | X=5], but since the output of the second dice is not influenced by the output of the first one, this is equal to P[X=5]*P[Y=2] .

--------------------------------------------------------------------------------------
Sources:

https://en.wikipedia.org/wiki/Marginal_distribution

https://online.stat.psu.edu/stat414/book/export/html/717

https://www.eolss.net/Sample-Chapters/C02/E6-02-04.pdf

https://en.wikipedia.org/wiki/Independence_(probability_theory)

https://en.wikipedia.org/wiki/Bayes%27_theorem
